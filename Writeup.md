# Assignment 2 Writeup

Kayla Anderson - Whitt Sanders - Arya Ray-Shryock

### Dataset Preparation
To prepare the dataset for finetuning, we created a preprocessing file. Our file takes a csv as input, strips all newlines from the cleaned_method column, and replaces the text representation of the tab spacing in the methods with a \<tab\> token to preserve python indentation. Since the values in the target_block column were already tokenized, we could not simply run a replace function on the cleaned_method column. To handle this, we created a list of all the values in the target_block column and stripped them of spaces. Then, we used a regex function to search each method for the target agnostic of spaces, and replaced the first instance of the target block with a \<IF-STMT\> token. After these processing steps, we saved the output in a new csv.

### Fine-tuning

### Evaluation Results

Evaluation of the model was done on Google Collab. The zipped model was uploaded along with the test set for evalutation. We looked at 4 different metrics; exact match (1 for true, 0 for false), CodeBleu, Bleu-4, and F1 score. CodeBleu was calculated using the code from Microsoft's CodeXGLUE repo, "https://github.com/microsoft/CodeXGLUE.git". CodeBleu is an combination of 4 different metrics, which we weighted equally (25% per metric). These are ngram match, weighted ngram match, syntax match, and data flow match. Bleu-4 was calculated using sacrebleu, which itself returns the Bleu-4 score. The F1 score was calculated using sklearn's f1 module; evaluating at a token level (not character level), and using the "macro" average (which means the metric is calculated for each label, and returns the unweighted mean). For example given the input method of, "def listdir(self, d):<tab>try:<tab><tab>return [<tab><tab><tab>p<tab><tab><tab>for p in os.listdir(d)<tab><tab><tab><IF-STMT><tab><tab>]<tab>except OSError:<tab><tab>return []", the model predicted "if os . path . isdir ( os . path . join ( d , p ) )", when the ground truth was "if os . path . basename ( p ) != "CVS" and os . path . isdir ( os . path . join ( d , p ) )". The scores for this particular case were exact match = 0 (not a match), codeBleu = 74.00, Bleu-4 = 45.94, and F1 = 0.2657. The average scores of our model for the entire test set were; an exact match 31.34% of the time, codeBleu = 88.53, Bleu-4 = 41.62, and F1 = 0.6350. Overall the model preformed fairly well, often returning an exact match or a similar prediction. The codeBleu score was particulary high, because in this case the entire method was used as input with either the predicted or ground truth; this was in order for the data flow match to be evaluted. This lead to a high score as many tokens were shared simply from the input method.
